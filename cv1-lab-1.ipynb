{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://d3i71xaburhd42.cloudfront.net/261c3e30bae8b8bdc83541ffa9331b52fcf015e6/3-Figure2-1.png\" width=50% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Lab 1: Colour, Intrinsic Image Decomposition & Photometric Stereo</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59, September 13, 2024 (Amsterdam time)</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\"><b>TA's: Xiaoyan, Floris, Adrian</b></font>\n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  \\\n",
    "Student1 Name: \n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name: \n",
    "\n",
    "Student3 ID: \\\n",
    "Student3 Name: \n",
    "\n",
    "( Student4 ID: \\\n",
    "Student4 Name: )\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"Python 3 or a more recent version is required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment and libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n",
    "warnings.simplefilter(action = \"ignore\", category = UserWarning)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you're using the provided environment!\n",
    "assert cv2.__version__ == \"4.10.0\", \"You're not using the provided Python environment!\"\n",
    "assert np.__version__ == \"1.26.4\", \"You're not using the provided Python environment!\"\n",
    "assert matplotlib.__version__ == \"3.9.2\", \"You're not using the provided Python environment!\"\n",
    "# Proceed to the next cell if you don't get any error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Instructions**\n",
    "\n",
    "Your code and discussion must be handed in this jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Lab 1 Assignment. Please also fill out your names and ID's above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "- It is mandatory to **use the Python environment provided** with the assignment; the environment specifies the package versions that have to be used to prevent the use of particular functions. Using different packages versions may lead to grade deduction. In the Python cell above you can check whether your environment is set up correctly.\n",
    "- To install the environment with the right package versions, use the following command in your terminal: ```conda env create --file=cv1_environment.yaml```, then activate the environment using the command ```conda activate cv1```.\n",
    "- Do not use additional packages or materials that have not been provided or explicitly mentioned.\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Answer all given questions and sub-questions.\n",
    "- Try to understand the problem as much as you can. When answering a question, give evidences (qualitative and/or quantitative results, references to papers, figures etc.) to support your arguments. Note that not everything might be explicitly asked for and you are expected to think about what might strengthen you arguments and make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Do not forget to add a number, a title, and if applicable name and unit of variables in a table, name and unit of axes and legends in a figure.\n",
    "\n",
    "__Note:__ A more complete overview of the lab requirements can be found in the Course Manual on Canvas\n",
    "\n",
    "Late submissions are not allowed. Assignments that are submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance, to avoid last minute system failure issues.\n",
    "\n",
    "Plagiarism note: Keep in mind that plagiarism (submitted materials which are not your work) is a serious crime and any misconduct shall be punished with the university regulations. This includes the use of generative tools such as ChatGPT.\n",
    "\n",
    "**ENSURE THAT YOU SAVE ALL RESULTS / ANSWERS ON THE QUESTIONS (EVEN IF YOU RE-USE SOME CODE).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Colour Spaces (13 points)](#section-1)\n",
    "  - [Question 1 (2 points)](#question-1)\n",
    "  - [Question 2 (6 points)](#question-2)\n",
    "  - [Question 3 (2 points)](#question-3)\n",
    "  - [Question 4 (2 points)](#question-4)\n",
    "  - [Question 5 (1 point)](#question-5)\n",
    "- [Section 2: Colour Constancy (13 points)](#section-2)\n",
    "  - [Question 6 (8 points)](#question-6)\n",
    "  - [Question 7 (2 points)](#question-7)\n",
    "  - [Question 8 (3 points)](#question-8)\n",
    "- [Section 3: Intrinsic Image Decomposition (14 points)](#section-3)\n",
    "  - [Question 9 (2 points)](#question-9)\n",
    "  - [Question 10 (2 points)](#question-10)\n",
    "  - [Question 11 (4 points)](#question-11)\n",
    "  - [Question 12 (2 points)](#question-12)\n",
    "  - [Question 13 (2 points)](#question-13)\n",
    "  - [Question 14 (2 points)](#question-14)\n",
    "- [Section 4: Photometric Stereo (60 points)](#section-4)\n",
    "  - [Question 15 (4 points)](#question-15)\n",
    "  - [Question 16 (2 points)](#question-16)\n",
    "  - [Question 17 (1 point)](#question-17)\n",
    "  - [Question 18 (2 points)](#question-18)\n",
    "  - [Question 19 (2 points)](#question-19)\n",
    "  - [Question 20 (5 points)](#question-20)\n",
    "  - [Question 21 (3 points)](#question-21)\n",
    "  - [Question 22 (6 points)](#question-22)\n",
    "  - [Question 23 (2 points)](#question-23)\n",
    "  - [Question 24 (3 points)](#question-24)\n",
    "  - [Question 25 (3 points)](#question-25)\n",
    "  - [Question 26 (2 points)](#question-26)\n",
    "  - [Question 27 (4 points)](#question-27)\n",
    "  - [Question 28 (5 points)](#question-28)\n",
    "  - [Question 29 (2 points)](#question-29)\n",
    "  - [Question 30 (2 points)](#question-30)\n",
    "  - [Question 31 (4 points)](#question-31)\n",
    "  - [Question 32 (1 point)](#question-32)\n",
    "  - [Question 33 (2 points)](#question-33)\n",
    "  - [Question 34 (2 points)](#question-34)\n",
    "  - [Question 35 (1 point)](#question-35)\n",
    "  - [Question 36 (4 points)](#question-36)\n",
    "  - [Question 37 (1 point)](#question-37)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "### **Section 1: Colour Spaces**\n",
    "\n",
    "In this part of the assignment, you will study the different colour spaces for image representations and experiment how to convert a given RGB image to a specific colour space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-1\"></a>\n",
    "#### <font color='#FF0000'>Question 1 (2 points)</font>\n",
    "\n",
    "Why do we use the RGB color model as the basis for our digital cameras and photography? How does a standard digital camera capture a full RGB color image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-2\"></a>\n",
    "#### <font color='#FF0000'>Question 2 (6 points)</font>\n",
    "\n",
    "Create a function to convert an RGB image into various color spaces using the provided `convert_colour_space()` function and other sub-functions.\n",
    "\n",
    "**Color Spaces to Convert:**\n",
    "\n",
    "1. **Grayscale**\n",
    "\n",
    "   Convert the RGB image into grayscale using 3 different methods mentioned in [John D. Cook's blog](https://www.johndcook.com/blog/2009/08/24/algorithms-convert-color-grayscale/). Additionally, check and report which method OpenCV uses for grayscale conversion, include it as well, and visualize all 4 methods in the same figure.\n",
    "\n",
    "2. **Opponent Color Space**\n",
    "\n",
    "   $\\begin{pmatrix}\n",
    "   O_1 \\\\\n",
    "   O_2 \\\\\n",
    "   O_3\n",
    "   \\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "   \\frac{R-G}{\\sqrt{2}} \\\\\n",
    "   \\frac{R+G-2B}{\\sqrt{6}} \\\\\n",
    "   \\frac{R+G+B}{\\sqrt{3}}\n",
    "   \\end{pmatrix}$\n",
    "\n",
    "3. **Normalized RGB (rgb) Color Space**\n",
    "\n",
    "   $\\begin{pmatrix}\n",
    "   r \\\\\n",
    "   g \\\\\n",
    "   b\n",
    "   \\end{pmatrix}$ = $\\begin{pmatrix}\n",
    "   \\frac{R}{R+G+B} \\\\\n",
    "   \\frac{G}{R+G+B} \\\\\n",
    "   \\frac{B}{R+G+B}\n",
    "   \\end{pmatrix}$\n",
    "\n",
    "4. **HSV Color Space**\n",
    "\n",
    "   Convert the RGB image into HSV Color Space using OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2HSV)*.\n",
    "\n",
    "5. **YCbCr Color Space**\n",
    "\n",
    "   Convert the RGB image into YCbCr Color Space using OpenCV’s built-in function *cv2.cvtColor(img, cv2.RGB2YCrCb)*. Arrange the channels in $Y, C_b,$ and $C_r$ order.\n",
    "\n",
    "**Note:** \n",
    "\n",
    "- Ensure you understand the data types and ranges required by the Python conversion and image displaying functions. This usually means [0, 1] for float datatype or [0, 255] for integer datatype. Explicitly change the datatype if necessary.\n",
    "\n",
    "- Each color space may have its own visualization requirements. For instance, think about how to meaningfully visualize the H, S, and V channels. Ensure each channel is visualized in an RGB manner where applicable.\n",
    "\n",
    "**Extra:** It might be interesting to visualize the image in its original color space (RGB) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_grays(input_image):\n",
    "    '''\n",
    "    Converts an RGB image into grayscale using different methods and \n",
    "    returns the grayscale images stacked as separate channels.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels representing different grayscale conversion methods\n",
    "    '''\n",
    "\n",
    "    # convert input image to array format if not already\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # lightness method\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # average method\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # luminosity method\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # built-in OpenCV function\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # stack the results to easily visualize the 4 different methods\n",
    "    new_image = np.stack([lightness, average, luminosity, opencv], axis=-1)\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_opponent(input_image):\n",
    "    '''\n",
    "    Converts an RGB image into the opponent color space and \n",
    "    returns the image with opponent color channels.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels representing opponent color space\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rgb_to_normedrgb(input_image):\n",
    "    '''\n",
    "    Converts an RGB image into the normalized RGB (nrgb) color space and \n",
    "    returns the image with normalized RGB channels.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels representing normalized RGB color space\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_colour_space(input_image, colourspace):\n",
    "    '''\n",
    "    Converts an RGB image into a specified color space and \n",
    "    returns the image in its new color space.\n",
    "\n",
    "    Args:\n",
    "        input_image: RGB image\n",
    "        colourspace: colour space to be converted to. \n",
    "        - choices: opponent, nrgb, hsv, ycbcr, grays\n",
    "\n",
    "    Returns:\n",
    "        new_image: image with channels in provided colour space\n",
    "    '''\n",
    "\n",
    "    # Convert the image into double precision for conversions\n",
    "    input_image = input_image.astype(np.float32)\n",
    "\n",
    "    if colourspace.lower() == 'opponent':\n",
    "        # fill in the rgb2opponent function\n",
    "\n",
    "        new_image = convert_rgb_to_opponent(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'nrgb':\n",
    "        # fill in the rgb2normedrgb function\n",
    "\n",
    "        new_image = convert_rgb_to_normedrgb(input_image)\n",
    "\n",
    "    elif colourspace.lower() == 'hsv':\n",
    "        # use built-in function from opencv\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif colourspace.lower() == 'ycbcr':\n",
    "        # use built-in function from opencv\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif colourspace.lower() == 'grays':\n",
    "        # fill in the rgb2grays function\n",
    "\n",
    "        new_image = convert_rgb_to_grays(input_image)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown colorspace type [%s]...' % colourspace)\n",
    "\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-3\"></a>\n",
    "#### <font color='#FF0000'>Question 3 (2 points)</font>\n",
    "\n",
    "Visualize the image in each new color space and its individual channels separately within the same figure. For example, for the HSV color space, visualize the converted HSV image and its Hue, Saturation, and Value channels separately (4 images in 1 figure). Ensure that the channel visualizations are **meaningful**.\n",
    "\n",
    "The function `visualize_colourspace` below needs to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_colourspace(image, colourspace='rgb'):\n",
    "    \"\"\"\n",
    "    Visualize the image in its corresponding colour space.\n",
    "    This function visualizes 1 figure with 4 images.\n",
    "    Hint: some RGB representation of the new colour space image + each of its channels\n",
    "\n",
    "    Args:\n",
    "        image: image with channels of some colour space\n",
    "        colourspace: colour space to be converted to. \n",
    "        - choices: opponent, nrgb, hsv, ycbcr, grays\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    if colourspace == 'opponent':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif colourspace in ['rgb', 'nrgb']:\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif colourspace == 'hsv':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif colourspace == 'ycbcr':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif colourspace == 'grays':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown colorspace type [%s]...' % colourspace)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and convert the image to RGB\n",
    "image = cv2.imread('images/beehive_house.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-4\"></a>\n",
    "#### <font color='#FF0000'>Question 4 (2 points)</font>\n",
    "\n",
    "Explain each of the above 5 colour spaces and their properties. What are the benefits of using a different colour space other than RGB? Provide reasons for each of the above cases. You can include your observations from the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-5\"></a>\n",
    "#### <font color='#FF0000'>Question 5 (1 point)</font>\n",
    "\n",
    "Find one more colour space from the literature, briefly explain its properties and give a use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Colour Constancy**\n",
    "\n",
    "Colour constancy is the ability to perceive the colors of objects consistently, regardless of the color of the light source. The goal of color constancy algorithms is to estimate the light source's illuminant and then correct the image so that it appears as if taken under a canonical (white) light source. In digital cameras, the automatic white balance (AWB) function performs this task to ensure images look natural.\n",
    "\n",
    "In this part of the assignment, you will implement the well-known Grey-World Algorithm, a fundamental color constancy algorithm. This algorithm operates under the assumption that, under white light, the average color in a scene should be grey ([128, 128, 128]).\n",
    "\n",
    "For more information, refer to the [Grey World algorithm on Wikipedia.](https://en.wikipedia.org/wiki/Color_normalization#Grey_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-6\"></a>\n",
    "#### <font color='#FF0000'>Question 6 (8 points)</font>\n",
    "\n",
    "Complete the function `apply_grey_world_algorithm` to apply color correction to an RGB image using the Grey-World algorithm.\n",
    "\n",
    "Display the original image and the color-corrected one on the same figure. Use the `beehive_house.jpg` image to test your algorithm. You should see that the reddish color cast on the image is removed, making it look more natural.\n",
    "\n",
    "***Note:*** You do not need to apply any pre or post-processing steps. For the calculation or processing, you are not allowed to use any available code or any dedicated library function except *standard Numpy functions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_grey_world_algorithm(awb_img):\n",
    "    \"\"\"\n",
    "    Apply the Grey-World algorithm to correct the color balance of an RGB image.\n",
    "\n",
    "    This function assumes that under a white light source, the average color in a scene should be grey.\n",
    "    It corrects the image by scaling each channel so that the mean of each channel is equal.\n",
    "\n",
    "    Args:\n",
    "        awb_img: RGB image to be color corrected\n",
    "\n",
    "    Returns:\n",
    "        gw_img: Color corrected image using the Grey-World algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert image to a numpy array\n",
    "    img_array = np.asarray(awb_img, dtype=np.float32)\n",
    "    \n",
    "    # Calculate the average of each channel\n",
    "    R_average = np.mean(img_array[:, :, 0])\n",
    "    G_average = np.mean(img_array[:, :, 1])\n",
    "    B_average = np.mean(img_array[:, :, 2])\n",
    "    \n",
    "    # Calculate the scaling factors for each channel\n",
    "    scale_R = 128 / R_average\n",
    "    scale_G = 128 / G_average\n",
    "    scale_B = 128 / B_average\n",
    "    \n",
    "    # Apply the scaling factors\n",
    "    img_array[:, :, 0] *= scale_R\n",
    "    img_array[:, :, 1] *= scale_G\n",
    "    img_array[:, :, 2] *= scale_B\n",
    "    \n",
    "    # Clipping ensures the pixel values remain within range [0, 255]\n",
    "    img_array = np.clip(img_array, 0, 255)\n",
    "    \n",
    "    # Convert the array back to an image\n",
    "    gw_img = np.uint8(img_array)\n",
    "\n",
    "    return gw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and convert the image to rgb\n",
    "image = cv2.imread('images/beehive_house.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "corrected_image = apply_grey_world_algorithm(image)\n",
    "\n",
    "# Display the original and corrected images side by side\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "# Corrected image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Corrected Image\")\n",
    "plt.imshow(corrected_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-7\"></a>\n",
    "#### <font color='#FF0000'>Question 7 (2 points)</font>\n",
    "\n",
    "Give an example case for Grey-World Algorithm on where it might fail. Include your reasoning.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Grey-World Algorithm might fail in situations where a single color dominates scene, such as in a green forest, during sunsets or underwater. The assumption that the average color in the image is grey does not hold in these cases. To still achieve this assumption, the algorithm will try to reduce the dominant color and artificially boost the other channels. This correction would result in an unnatural color shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-8\"></a>\n",
    "#### <font color='#FF0000'>Question 8 (3 points)</font>\n",
    "\n",
    "Find out one more colour constancy algorithm from the literature and explain it briefly.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(REVISE THIS ONE)\n",
    "Literature: Forsyth, D. A. (1990). A novel algorithm for color constancy. International Journal of Computer Vision, 5(1), 5-36.\n",
    "\n",
    "Color constancy algorithms in e.g. computer vision are used to ensure that colors in images appear as they would under natural lighting conditions, even while illuminated by coloured light sources. They work by estimating the color of the light source, calculating scaling factors, and applying these corrections to produce a color-consistent image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: Intrinsic Image Decomposition**\n",
    "\n",
    "Intrinsic image decomposition is the process of separating an image into its formation components, such as reflectance (albedo) and shading (illumination). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) Then, under the assumptions of body (diffuse) reflection, linear sensor response and narrow band filters, the decomposition of the observed image $I(\\vec{x})$ at position $\\vec{x}$ can be approximated as the element-wise product of its albedo $R(\\vec{x})$ and shading $S(\\vec{x})$ intrinsics:\n",
    "\n",
    "$$I(\\vec{x})=R(\\vec{x}) \\times S(\\vec{x})$$\n",
    "\n",
    "In this part of the assignment, you will experiment with intrinsic image components to perform a particular computational photography application: material recolouring. For the experiments, we will use images from a synthetic intrinsic image dataset. <a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2)\n",
    "\n",
    "<a name=\"cite_note-1\"></a><small>[1.](#cite_ref-1) H. G. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems, pages 3-26, 1978.</small>\n",
    "\n",
    "<a name=\"cite_note-2\"></a><small>[2.](#cite_ref-1) http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-9\"></a>\n",
    "#### <font color='#FF0000'>Question 9 (2 points)</font>\n",
    "\n",
    "In what other components can an image be decomposed other than albedo and shading? Give an example and explain the concepts in your answer.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating the channels of an image\n",
    "For example, we can separate an image into its RGB components or HSV components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-10\"></a>\n",
    "#### <font color='#FF0000'>Question 10 (2 points)</font>\n",
    "\n",
    "If you check the literature, you will see that almost all intrinsic image decomposition datasets are composed of synthetic images. What might be the reason for that?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because in virtual environment we have complete control over the scene's parameters such as lighting and textures. This control translates into knowledge: knowing the ground truth regarding the scene allow to validate and tune algorithms that uses this data. This is of course much more difficult to do with \"real\" pictures because of all the unknown variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-11\"></a>\n",
    "#### <font color='#FF0000'>Question 11 (4 points)</font>\n",
    "\n",
    "Choose an object with a single color from the `/intrinsic_images/` folder, which is based on the [synthetic intrinsic image dataset](http://www.cic.uab.cat/Datasets/synthetic_intrinsic_image_dataset/).\n",
    "\n",
    "Demonstrate that you can reconstruct the original PNG image from its intrinsics using the albedo and shading. Your script should output a figure displaying the original image, its intrinsic images, and the reconstructed one. Complete the function `reconstruct_image_from_intrinsics()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image_from_intrinsics(albedo_img, shading_img):\n",
    "    \"\"\"\n",
    "    Reconstruct the original image from its intrinsic images (albedo and shading).\n",
    "\n",
    "    This function takes the albedo and shading images as input, \n",
    "    and reconstructs the original image by multiplying albedo with shading.\n",
    "\n",
    "    Args:\n",
    "        albedo_img: Image representing the albedo (reflectance)\n",
    "        shading_img: Image representing the shading\n",
    "\n",
    "    Returns:\n",
    "        iid_img: Reconstructed image\n",
    "    \"\"\"\n",
    "    # Convert images to float\n",
    "    if albedo_img.dtype != np.float32:\n",
    "        albedo_img = albedo_img.astype(np.float32) / 255\n",
    "    if shading_img.dtype != np.float32:\n",
    "        shading_img = shading_img.astype(np.float32) / 255\n",
    "\n",
    "    # Element-wise multiplication, we miss a constant factor Kd\n",
    "    kd = 1\n",
    "    iid_img = kd * albedo_img * shading_img\n",
    "    \n",
    "    # Scale back to 0-255 if needed\n",
    "    iid_img = np.clip(iid_img * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return iid_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the object name with a valid name from the dataset\n",
    "img_path = './intrinsic_images/'\n",
    "object_name = 'turt_ml1'\n",
    "\n",
    "# Read with opencv\n",
    "obj = cv2.imread(img_path + object_name + '.png')\n",
    "if obj is None:\n",
    "    raise FileNotFoundError(f'No image found with filename: {img_path + object_name + \".png\"}')\n",
    "else:\n",
    "    print(f'object shape: {obj.shape}')\n",
    "\n",
    "obj_sha = cv2.imread(img_path + object_name + '_shad.png')\n",
    "if obj_sha is None:\n",
    "    raise FileNotFoundError(f'No image found with filename: {img_path + object_name + \"_shad.png\"}')\n",
    "else:\n",
    "    print(f'object albedo shape: {obj_sha.shape}')\n",
    "\n",
    "obj_alb = cv2.imread(img_path + object_name.split('_')[0] + '_refl.png')\n",
    "if obj_alb is None:\n",
    "    raise FileNotFoundError(f'No image found with filename: {img_path + object_name.split(\"_\")[0] + \"_refl.png\"}')\n",
    "else:\n",
    "    print(f'object shading shape: {obj_alb.shape}')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Convert images to RGB\n",
    "obj = cv2.cvtColor(obj, cv2.COLOR_BGR2RGB)\n",
    "obj_alb_rgb = cv2.cvtColor(obj_alb, cv2.COLOR_BGR2RGB)\n",
    "obj_sha_rgb = cv2.cvtColor(obj_sha, cv2.COLOR_BGR2RGB)\n",
    "reconstructed_image = reconstruct_image_from_intrinsics(obj_alb_rgb, obj_sha_rgb)\n",
    "\n",
    "# Display reconstructed image, original image, albedo and shading images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "axes[0].imshow(obj)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(obj_alb_rgb)\n",
    "axes[1].set_title('Albedo Image')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(obj_sha_rgb)\n",
    "axes[2].set_title('Shading Image')\n",
    "axes[2].axis('off')\n",
    "axes[3].imshow(reconstructed_image)\n",
    "axes[3].set_title('Reconstructed Image')\n",
    "axes[3].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-12\"></a>\n",
    "#### <font color='#FF0000'>Question 12 (2 points)</font>\n",
    "\n",
    "Manipulating colors in photographs is an important problem with many applications in computer vision. Recoloring algorithms aim to manipulate colors effectively, and better results can be obtained if the albedo image is available as it is independent of confounding illumination effects.\n",
    "\n",
    "First determine the true material color of the object you picked in RGB space (uniform color in this case). \n",
    "\n",
    "- Complete the code for the function `get_true_color()`.\n",
    "- Plot the true color of the object in RGB space, make sure to also plot each channel separately. \n",
    "\n",
    "**Hint:** You can use `np.tile` to create a visualization of the true color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_color(albedo_img):\n",
    "    \"\"\"\n",
    "    Determines the true material color of the object in the albedo image by calculating the average RGB color.\n",
    "\n",
    "    Args:\n",
    "        albedo_img: Albedo (reflectance) image as a NumPy array with shape (height, width, 3).\n",
    "\n",
    "    Returns:\n",
    "        true_color: True material color of the object in RGB space as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Ensure the image is in float format for accurate color averaging\n",
    "    if albedo_img.dtype != np.float32 and albedo_img.dtype != np.float64:\n",
    "        albedo_img = albedo_img.astype(np.float32) / 255\n",
    "\n",
    "    # Create a mask to filter out black pixels (0, 0, 0)\n",
    "    mask = (albedo_img > 0).any(axis=2)\n",
    "\n",
    "    # Apply the mask to filter out the black pixels\n",
    "    filtered_img = albedo_img[mask]\n",
    "\n",
    "    # Compute the mean along the color channels\n",
    "    if filtered_img.size > 0:  # Ensure there are non-black pixels\n",
    "        true_color = np.mean(filtered_img, axis=0)\n",
    "    else:\n",
    "        true_color = np.array([0, 0, 0], dtype=np.float32)  # Return black if all pixels were black\n",
    "    \n",
    "    # Scale back to 0-255 if needed\n",
    "    true_color = np.clip(true_color * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return true_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the true color of the object's albedo image\n",
    "obj_alb_rgb = cv2.cvtColor(obj_alb, cv2.COLOR_BGR2RGB)\n",
    "true_color = get_true_color(obj_alb_rgb)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 15))\n",
    "\n",
    "# Plot the true color\n",
    "true_img = np.ones((100, 100, 3), dtype=np.uint8) * true_color\n",
    "axes[0].imshow(true_img)\n",
    "axes[0].set_title('True Color')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot red channel\n",
    "red_channel = np.ones((100, 100), dtype=np.uint8) * true_color[0]\n",
    "axes[1].imshow(red_channel, cmap='gray', vmin=0, vmax=255)\n",
    "axes[1].set_title('Red Channel intensity')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Plot green channel\n",
    "green_channel = np.ones((100, 100), dtype=np.uint8) * true_color[1]\n",
    "axes[2].imshow(green_channel, cmap='gray', vmin=0, vmax=255)\n",
    "axes[2].set_title('Green Channel intensity')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# Plot blue channel\n",
    "blue_channel = np.ones((100, 100), dtype=np.uint8) * true_color[2]\n",
    "axes[3].imshow(blue_channel, cmap='gray', vmin=0, vmax=255)\n",
    "axes[3].set_title('Blue Channel intensity')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-13\"></a>\n",
    "#### <font color='#FF0000'>Question 13 (2 points)</font>\n",
    "\n",
    "Assume you are given the PNG image and have access to its intrinsic albedo and shading images.\n",
    "\n",
    "Recolor the object's image with pure red (255, 0, 0). Complete the code for the function `recolor_image()`. Display the recolored version of the object. Make sure to also plot each channel separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recolor_image(albedo_img, shading_img):\n",
    "    \"\"\"\n",
    "    Recolors an image based on its albedo and shading images.\n",
    "\n",
    "    This function recolors the input albedo image by changing non-black pixels to red\n",
    "    and then reconstructs the final image using the provided shading image.\n",
    "\n",
    "    Args:\n",
    "        albedo_img: Albedo (reflectance) image\n",
    "        shading_img: Shading image\n",
    "\n",
    "    Returns:\n",
    "        recolored_img: Recolored and reconstructed image\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Convert images to float\n",
    "    if albedo_img.dtype != np.float32:\n",
    "        albedo_img = albedo_img.astype(np.float32) / 255\n",
    "\n",
    "    albedo_img_red = albedo_img.copy()\n",
    "\n",
    "    # Assuming albedo_img_red is your n*m*3 numpy array\n",
    "    # Step 1: Create a mask where any channel is non-zero\n",
    "    mask = (albedo_img_red != 0).any(axis=2)\n",
    "\n",
    "    # Step 2: Use the mask to set the entire pixel to red\n",
    "    albedo_img_red[mask] = [1, 0, 0]  # Change non-black pixels to red\n",
    "\n",
    "    recolored_img = reconstruct_image_from_intrinsics(albedo_img_red, shading_img)\n",
    "    \n",
    "    return recolored_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recolored_image = recolor_image(obj_alb, obj_sha)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Plot image and each separate channel\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "axes[0].imshow(recolored_image)\n",
    "axes[0].set_title('Recolored Image')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(recolored_image[:, :, 0], cmap='gray')\n",
    "axes[1].set_title('Red Channel')\n",
    "axes[1].axis('off')\n",
    "axes[2].imshow(recolored_image[:, :, 1], cmap='gray')\n",
    "axes[2].set_title('Green Channel')\n",
    "axes[2].axis('off')\n",
    "axes[3].imshow(recolored_image[:, :, 2], cmap='gray')\n",
    "axes[3].set_title('Blue Channel')\n",
    "axes[3].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-14\"></a>\n",
    "#### <font color='#FF0000'>Question 14 (2 points)</font>\n",
    "\n",
    "Although you have recoloured the object with pure red, the reconstructed images do not seem to display those pure colors and thus the colour distributions over the object do not appear uniform. Explain the reason.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not display the true colors for the simple reason that we miss the scale factor Kd in the formula which represents the fraction of incoming light that gets reflected. We played with the Kd value in the reconstruct_image_from_intrinsics function and obtained better results by increasing Kd to 1.8 (it is really close to the original image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__  This was a simple case where the image is synthetic, object centered and has only one colour, and you have access to its ground-truth intrinsic images. Real world scenarios require more than just replacing a single colour with another, not to mention the complexity of achieving a decent intrinsic image decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: Photometric Stereo**\n",
    "\n",
    "In this section, we delve into the photometric stereo technique, a method used to reconstruct a surface's shape from a set of images captured under varying lighting conditions. This approach is based on the principles outlined in Section 5.4 of Forsyth and Ponce's *Computer Vision: A Modern Approach*, which provides a comprehensive introduction to the theory and application of photometric stereo in computer vision.\n",
    "\n",
    "Photometric stereo is particularly useful for capturing fine details of surface geometry. The core idea is to utilize an orthographic camera to take multiple images of a surface, each under different lighting conditions. By analyzing the variations in image intensity across these images, we can infer the surface normals and, subsequently, reconstruct the surface's height at each pixel, leading to what is commonly known as a height map or depth map.\n",
    "\n",
    "The method assumes that the camera and the surface remain stationary, with illumination coming from different directions. By capturing enough images with different light source vectors, we can solve for the surface normals and albedo at each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-15\"></a>\n",
    "#### <font color='#FF0000'>Question 15 (4 points)</font>\n",
    "\n",
    "Start with the grayscale sphere model located in the SphereGray5 folder. The folder contains 5 images of a sphere with a grayscale checker texture under different lighting conditions. Your task is to estimate the surface reflectance (albedo) and surface normal of this model.\n",
    "\n",
    "In this question, you will need to complete the function `estimate_albedo_and_normals()` to estimate the albedo and surface normal map for the SphereGray5 folder.\n",
    "\n",
    "To assist you, two helper functions are provided:\n",
    "\n",
    "- `load_synthetic_images()`: This function loads the synthetic images from the specified directory.\n",
    "- `show_results()`: This function displays the albedo, surface normals, and optionally the height map and surface error.\n",
    "\n",
    "These helper functions are provided to simplify the process of loading images and displaying your results.\n",
    "\n",
    "**Hint**: To get the least-squares solution of a linear system, you can use the `numpy.linalg.lstsq` function.\n",
    "\n",
    "Make sure to include images of your results in your notebook at key points. When visualizing 3D models, choose viewpoints that clearly illustrate the structure, and feel free to use multiple viewpoints if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_images(image_dir='./photometrics_images/SphereGray5/', channel=0):\n",
    "    \"\"\"\n",
    "    Loads synthetic images and their corresponding light source directions.\n",
    "\n",
    "    This function reads all images from the specified directory, extracts the relevant channel, and stacks the images along the third dimension. It also extracts the light source directions from the image filenames and normalizes both the image stack and the light source vectors.\n",
    "\n",
    "    Args:\n",
    "        image_dir: directory containing the synthetic images\n",
    "        channel: color channel to be extracted (default is 0, corresponding to the red channel)\n",
    "\n",
    "    Returns:\n",
    "        image_stack: a 3d numpy array of stacked images\n",
    "        normalized_light_dirs: a 2d numpy array of normalized light source directions\n",
    "    \"\"\"\n",
    "    # list all files in the image directory\n",
    "    image_files = os.listdir(image_dir)\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    image_stack = None\n",
    "    light_directions = None\n",
    "    \n",
    "    # assumed z component for the light direction\n",
    "    z_component = 0.5  \n",
    "\n",
    "    # loop through all image files\n",
    "    for i in range(num_images):\n",
    "        # read the input image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_files[i]))\n",
    "        image = np.flip(image, axis=-1)  # flip the image to correct the channel order\n",
    "        image = image[:, :, channel]  # extract the specified color channel\n",
    "\n",
    "        # initialize image stack and light direction array on the first iteration\n",
    "        if image_stack is None:\n",
    "            height, width = image.shape\n",
    "            print('image size (h*w): %d*%d' % (height, width))\n",
    "            image_stack = np.zeros([height, width, num_images], dtype=int)\n",
    "            light_directions = np.zeros([num_images, 3], dtype=np.float64)\n",
    "\n",
    "        # stack the image along the third dimension\n",
    "        image_stack[:, :, i] = image\n",
    "\n",
    "        # extract light direction from the image filename\n",
    "        x_component = np.double(image_files[i][(image_files[i].find('_') + 1):image_files[i].rfind('_')])\n",
    "        y_component = np.double(image_files[i][image_files[i].rfind('_') + 1:image_files[i].rfind('.png')])\n",
    "        light_directions[i, :] = [x_component, -y_component, z_component]\n",
    "\n",
    "    # convert the image stack to double precision for further processing\n",
    "    image_stack = np.double(image_stack)\n",
    "\n",
    "    # normalize the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "\n",
    "    # handle the case where all pixel values are the same\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  \n",
    "\n",
    "    # normalize the light direction vectors\n",
    "    norm_factors = np.tile(np.sqrt(np.sum(light_directions ** 2, axis=1, keepdims=True)), (1, light_directions.shape[1]))\n",
    "    normalized_light_dirs = light_directions / norm_factors\n",
    "\n",
    "    return image_stack, normalized_light_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(albedo, normals, height_map=None, SE=None):\n",
    "    \"\"\"\n",
    "    Displays the albedo, normals, and optionally the height map and surface error (SE).\n",
    "\n",
    "    Args:\n",
    "        albedo: albedo (reflectance) map\n",
    "        normals: surface normal map\n",
    "        height_map: reconstructed height map (optional)\n",
    "        SE: surface error map (optional)\n",
    "    \"\"\"\n",
    "    # stride in the plot, you may want to adjust it to different images\n",
    "    stride = 1\n",
    "\n",
    "    if albedo is not None:\n",
    "        # showing albedo map\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        albedo_max = 1  # normalize albedo values\n",
    "        albedo = albedo / albedo_max\n",
    "        print(\"Albedo shape:\", albedo.shape)\n",
    "        plt.imshow(albedo, cmap=\"gray\")\n",
    "        plt.title('Albedo Map', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "    # showing normals as three separate channels\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    ax1 = figure.add_subplot(221)\n",
    "    ax1.imshow(normals[..., 0], vmin=-1, vmax=1, cmap='viridis')\n",
    "    ax1.set_axis_off()\n",
    "    ax1.set_title('Normal Channel x', fontsize=16)\n",
    "    ax2 = figure.add_subplot(222)\n",
    "    ax2.imshow(normals[..., 1], vmin=-1, vmax=1, cmap='viridis')\n",
    "    ax2.set_axis_off()\n",
    "    ax2.set_title('Normal Channel y', fontsize=16)\n",
    "    ax3 = figure.add_subplot(223)\n",
    "    ax3.imshow(normals[..., 2], vmin=-1, vmax=1, cmap='viridis')\n",
    "    ax3.set_axis_off()\n",
    "    ax3.set_title('Normal Channel z', fontsize=16)\n",
    "    ax4 = figure.add_subplot(224)\n",
    "    ax4.imshow((normals + 1) * 0.5)\n",
    "    ax4.set_axis_off()\n",
    "    ax4.set_title('Combined Normals', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    # Create meshgrid for plotting 3D surfaces\n",
    "    X, Y, _ = np.meshgrid(np.arange(0, np.shape(normals)[0], stride),\n",
    "                          np.arange(0, np.shape(normals)[1], stride),\n",
    "                          np.arange(1))\n",
    "    X = X[..., 0]\n",
    "    Y = Y[..., 0]\n",
    "\n",
    "    # Show both heightmap and SE in a 1x2 subplot if they're available\n",
    "    if height_map is not None and SE is not None:\n",
    "        fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "        # Show heightmap\n",
    "        ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        H_height_map = height_map[::stride, ::stride]\n",
    "        ax1.plot_surface(X, Y, H_height_map.T)\n",
    "        ax1.set_title(\"Height Map\")\n",
    "\n",
    "        # Show SE\n",
    "        ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "        H_SE = SE[::stride, ::stride]\n",
    "        ax2.plot_surface(X, Y, H_SE.T, color='r')\n",
    "        ax2.set_title(\"SE\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Only show heightmap if it's available\n",
    "    elif height_map is not None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        H_height_map = height_map[::stride, ::stride]\n",
    "        ax.plot_surface(X, Y, H_height_map.T)\n",
    "        ax.set_title(\"Height Map\")\n",
    "        plt.show()\n",
    "\n",
    "    # Only show SE if it's available\n",
    "    elif SE is not None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "        H_SE = SE[::stride, ::stride]\n",
    "        ax.plot_surface(X, Y, H_SE.T, color='r')\n",
    "        ax.set_title(\"SE\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the guidelines in *Computer Vision: A Modern Approach* by Forsyth and Ponce, Chapter 5, Section 5.4, you will complete the `estimate_albedo_and_normals()` function by implementing the following steps:\n",
    "\n",
    "1. **Iterate through each point in the image array**:\n",
    "   - For each pixel, extract the intensity values across all images and stack them into a vector `i`.\n",
    "\n",
    "2. **Construct the diagonal matrix**:\n",
    "   - Construct the diagonal matrix `scriptI` using the vector `i`.\n",
    "\n",
    "3. **Solve the linear system**:\n",
    "   - Solve the equation `scriptI * scriptV * g = scriptI * i` to obtain the vector `g` for each pixel.\n",
    "\n",
    "4. **Compute the albedo**:\n",
    "   - The albedo at each pixel is the magnitude (norm) of the vector `g`, denoted as `|g|`.\n",
    "\n",
    "5. **Compute the surface normal**:\n",
    "   - The surface normal at each pixel is given by `g / |g|`, where `|g|` is non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_albedo_and_normals(image_stack, scriptV, shadow_trick=True):\n",
    "    \"\"\"\n",
    "    Estimates the surface albedo and normals from a stack of images.\n",
    "\n",
    "    This function computes the albedo and surface normals for each pixel in the \n",
    "    image stack using the provided light source directions. Optionally, it applies \n",
    "    the shadow trick to improve the robustness of the solution.\n",
    "\n",
    "    Args:\n",
    "        image_stack: The stack of images of the surface, stacked along the third dimension.\n",
    "        scriptV: The matrix containing the source and camera information.\n",
    "        shadow_trick: Boolean indicating whether to use the shadow trick (default is True).\n",
    "\n",
    "    Returns:\n",
    "        albedo: The estimated surface albedo.\n",
    "        normal: The estimated surface normals.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, _ = image_stack.shape\n",
    "\n",
    "    # create arrays for the albedo and normals\n",
    "    albedo = np.zeros([h, w])\n",
    "    normal = np.zeros([h, w, 3])\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Convert the images to floating point\n",
    "    #if image_stack.dtype != np.float32:\n",
    "    #    image_stack = image_stack.astype(np.float32)\n",
    "\n",
    "    for x in range(h):\n",
    "        for y in range(w):\n",
    "            # Extract the intensity vector i for each pixel across all images\n",
    "            i = image_stack[x, y, :]\n",
    "\n",
    "            # Construct the diagonal matrix scriptI using vector i\n",
    "            scriptI = np.diag(i)\n",
    "\n",
    "            if shadow_trick:\n",
    "                # Using the shadow trick and the method provided in the question description and in the book\n",
    "                g = np.linalg.pinv (scriptI @ scriptV) @ scriptI @ i\n",
    "            else:\n",
    "                # Using the method provided on the slides\n",
    "                g = (i @ scriptV) @ np.linalg.inv(scriptV.T @ scriptV)\n",
    "            \n",
    "            # Compute the albedo at each pixel as the magnitude of g\n",
    "            albedo[x, y] = np.linalg.norm(g)\n",
    "\n",
    "            # Compute the surface normal at each pixel\n",
    "            if albedo[x, y] > 0:  # Avoid division by zero\n",
    "                normal[x, y, :] = g / albedo[x, y]\n",
    "            else:\n",
    "                normal[x, y, :] = np.zeros(3)\n",
    "        \n",
    "    return albedo, normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic images for SphereGray5\n",
    "image_stack, light_dirs = load_synthetic_images()\n",
    "# estimate albedo and normals\n",
    "albedo, normals = estimate_albedo_and_normals(image_stack, light_dirs)\n",
    "\n",
    "# show results\n",
    "show_results(albedo, normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-16\"></a>\n",
    "#### <font color='#FF0000'>Question 16 (2 points)</font>\n",
    "\n",
    "After implementing the `estimate_albedo_and_normals()` function, what do you expect to see in the albedo image? How does this compare to your actual result? Explain the differences.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expected exactly this result:\n",
    "\n",
    "Normal Channel X: on the left the sphere is darker, which means the normal points to the negative x-axis. In the center is close to 0, hence it is greenish which is the color in the middle between purple and yellow in the viridis cmap and on the right it's yellow which means the normal points to the right (stronger x positive component).\n",
    "\n",
    "Normal Channel Y: same reasoning as Channel X.\n",
    "\n",
    "Normal Channel Z: Naturally most of the normals have a very strong z component.\n",
    "\n",
    "Combined normals: It's an RGB image, the red is given by the Normal Channel X (the stronger the X component is the redder the combined image is), the green is given by the Normal Channel Y and the Blue is given by the Z channel, as you can see the image appears blue-ish since it's a sphere and it has a strong Z component.>*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-17\"></a>\n",
    "#### <font color='#FF0000'>Question 17 (1 point)</font>\n",
    "\n",
    "In principle, what is the minimum number of images you need to estimate the albedo and surface normal? Explain your reasoning.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum number is 3 since we have only 4 unknown variables for each pixel which are albedo and nx, ny and nz. Since the normal is a vector of length one, this constraint basically removes one unknown variable, so we only have 3 unknown variables which can be found with a system of three linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-18\"></a>\n",
    "#### <font color='#FF0000'>Question 18 (2 points)</font>\n",
    "\n",
    "Now, run the algorithm with more images by using the SphereGray25 folder. Observe the differences in the results when using more images. Report your findings. \n",
    "\n",
    "You could try all images at once or a few at a time, incrementally. Choose a strategy and justify it by discussing your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic images for SphereGray25\n",
    "image_stack, light_dirs = load_synthetic_images('./photometrics_images/SphereGray25/')\n",
    "\n",
    "# estimate albedo and normals for SphereGray25\n",
    "albedo, normals = estimate_albedo_and_normals(image_stack, light_dirs)\n",
    "\n",
    "# show the results for SphereGray25\n",
    "show_results(albedo, normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We couldn't see any difference between 25 images and 5 images when using the shadow trick. Instead, without using the shadow trick there is a big difference between 5 and 35 images, look at the next answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-19\"></a>\n",
    "#### <font color='#FF0000'>Question 19 (2 points)</font>\n",
    "\n",
    "Consider the impact of shadows in photometric stereo. Explain the trick mentioned in the textbook to deal with shadows.\n",
    "\n",
    "Remove this trick from your implementation and check your results. Is the trick necessary when using 5 images? How about when using 25 images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load synthetic images for SphereGray5 and SphereGray25\n",
    "image_stack_s5, light_dirs_s5 = load_synthetic_images('./photometrics_images/SphereGray5/')\n",
    "image_stack_s25, light_dirs_s25 = load_synthetic_images('./photometrics_images/SphereGray25/')\n",
    "\n",
    "# estimate albedo and normals without shadow trick\n",
    "albedo_s5, normals_s5 = estimate_albedo_and_normals(image_stack_s5, light_dirs_s5, shadow_trick=False)\n",
    "albedo_s25, normals_s25 = estimate_albedo_and_normals(image_stack_s25, light_dirs_s25, shadow_trick=False)\n",
    "\n",
    "# show results without shadow trick\n",
    "show_results(albedo_s5, normals_s5)\n",
    "show_results(albedo_s25, normals_s25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is absolutely necessary when using only 5 images since we can clearly see the artifcats in the normal maps given by the shadows boundary. When using 25 images we couldn't notice any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-20\"></a>\n",
    "#### <font color='#FF0000'>Question 20 (5 points)</font>\n",
    "\n",
    "Before reconstructing the surface height map, it is necessary to compute the partial derivatives, $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ (referred to as *p* and *q* in the algorithm). These partial derivatives also allow for a double-check of the computation through a test of *integrability*.\n",
    "\n",
    "In this question, you will need to complete the function `check_integrability()` to compute the partial derivatives *p* and *q*.\n",
    "\n",
    "Make sure to verify your results by examining the integrability of the computed derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrability(normals):\n",
    "    \"\"\"\n",
    "    Checks the surface gradient for integrability.\n",
    "\n",
    "    This function computes the partial derivatives of the surface function with respect to x and y \n",
    "    (p and q), and calculates the squared errors of the mixed partial derivatives to test for integrability.\n",
    "\n",
    "    Args:\n",
    "        normals: The normal map of the surface.\n",
    "\n",
    "    Returns:\n",
    "        p: The partial derivative of the surface function with respect to x (df/dx).\n",
    "        q: The partial derivative of the surface function with respect to y (df/dy).\n",
    "        SE: The squared errors of the mixed partial derivatives.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize p, q, and SE\n",
    "    p = np.zeros(normals.shape[:2])\n",
    "    q = np.zeros(normals.shape[:2])\n",
    "    SE = np.zeros(normals.shape[:2])\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # nx / nz means the derivative of the surface in x direction (how much the height changes in x direction), similarly ny / nz\n",
    "    # Compute the partial derivatives of the surface function with respect to x and y\n",
    "    n_x = normals[:, :, 0]\n",
    "    n_y = normals[:, :, 1]\n",
    "    n_z = normals[:, :, 2]\n",
    "    \n",
    "    # Avoid division by zero by masking small values of n_z\n",
    "    epsilon = 1e-6  # Small threshold to avoid division by very small n_z values\n",
    "    n_z_safe = np.where(np.abs(n_z) < epsilon, np.nan, n_z)\n",
    "    \n",
    "    # Surface gradients p and q\n",
    "    p = n_x / n_z_safe  # Gradient in the x direction\n",
    "    q = n_y / n_z_safe  # Gradient in the y direction\n",
    "    \n",
    "    # Remove NaN values from p and q\n",
    "    p = np.nan_to_num(p, nan=0)\n",
    "    q = np.nan_to_num(q, nan=0)\n",
    "\n",
    "    # Compute mixed partial derivatives\n",
    "    dp_dy = np.gradient(p, axis=0)  # Partial derivative of p with respect to y\n",
    "    dq_dx = np.gradient(q, axis=1)  # Partial derivative of q with respect to x\n",
    "    \n",
    "    # Squared error between the mixed partial derivatives\n",
    "    SE = (dp_dy - dq_dx) ** 2\n",
    "        \n",
    "    return p, q, SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the normals that you found in the previous question\n",
    "p, q, SE = check_integrability(normals_s5)\n",
    "\n",
    "print('Showing the integrability check results for 5 light sources:')\n",
    "print(f'SE max: {SE.max()}\\n')\n",
    "\n",
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "print('Showing the integrability check results for 25 light sources:')\n",
    "print(f'SE max: {SE.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-21\"></a>\n",
    "#### <font color='#FF0000'>Question 21 (3 points)</font>\n",
    "\n",
    "Implement and compute the second derivatives according to the provided algorithm, and perform the test of integrability by applying a reasonable threshold to the squared errors. \n",
    "\n",
    "Reflect on the potential causes of any errors observed. Additionally, analyze how the test of integrability performs when using the GraySphere5 and GraySphere25 datasets. Use the normal maps obtained from the previous steps with the shadow trick applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from matplotlib import cm\n",
    "\n",
    "def test_integrability(SE, threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Create a color-coded mask for integrability errors.\n",
    "    \n",
    "    Args:\n",
    "        SE: A (h, w) array representing the squared errors.\n",
    "        threshold: A threshold for integrability (default is 1e-3).\n",
    "    \n",
    "    Returns:\n",
    "        A color-coded image showing the severity of non-integrable regions.\n",
    "    \"\"\"\n",
    "    # Normalize SE values to range between 0 and 1\n",
    "    max_error = np.max(SE)\n",
    "    normalized_SE = SE / max_error if max_error != 0 else SE  # Avoid division by 0\n",
    "\n",
    "    colormap = cm.get_cmap('YlOrRd')\n",
    "    colored_mask = colormap(normalized_SE)\n",
    "\n",
    "    # Mark pixels below the threshold (which are considered integrable) with a different color\n",
    "    integrable_mask = SE <= threshold  # True for integrable pixels\n",
    "    \n",
    "    # Convert the integrable pixels to a distinct color\n",
    "    colored_mask[integrable_mask] = [0.0, 0.0, 0.0, 1.0]  # (RGBA)\n",
    "\n",
    "    return colored_mask\n",
    "\n",
    "non_integrable_mask_s5 = test_integrability(SE_s5, threshold=1e-4)\n",
    "non_integrable_mask_s25 = test_integrability(SE_s25, threshold=1e-4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "# Visualize the non-integrable regions\n",
    "# The redder the pixels are, the \"less integrable\" they are (higher error)\n",
    "# If they are white or yellow, it means lower error (more integrable)\n",
    "# If it is black it means it is integrable (below the threshold)\n",
    "axes[0].imshow(non_integrable_mask_s5)\n",
    "axes[0].set_title('Non-Integrable Regions (5 light sources)')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(non_integrable_mask_s25)\n",
    "axes[1].set_title('Non-Integrable Regions (25 light sources)')\n",
    "axes[1].axis('off')\n",
    "plt.suptitle('Mask for Integrability Errors, same threshold for both images', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors, with a threshold of 1e-3 are seen primarily on the edges of the sphere, we observe that the image made of only 5 light sources has more errors. Moreover, if we set the threshold even lower (1e-5), we can see the shadows of 4 out of the 5 light sources (meaning that even though the shadow trick works, the errors are still slightly visible). In both the images we can see that in the boundaries of the two colors that make up the image there is a slight error.\n",
    "\n",
    "These errors are all very understandable and predictable: the non integrable parts of the image are on the boundaries of the image (where there are color differences). These color differences are given by: albedo, shadows boundaries of the light source, foreground/background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-22\"></a>\n",
    "#### <font color='#FF0000'>Question 22 (6 points)</font>\n",
    "\n",
    "To reconstruct the surface height map, you need to integrate the partial derivatives over a path. Since we are working with discrete structures, this integration is done by summing their values.\n",
    "\n",
    "The algorithm presented in the chapter suggests performing the integration in a column-major order: start at the top-left corner, integrate along the first column, and then proceed to the right along each row. However, it is also recommended to use multiple paths and average the results to help distribute the errors from derivative estimates.\n",
    "\n",
    "In this question, you will:\n",
    "\n",
    "1. Construct the surface height map using column-major order as described in the algorithm.\n",
    "2. Implement the row-major path integration method.\n",
    "\n",
    "Your implementation should be added to the `construct_surface()` function.\n",
    "\n",
    "**Note**: By default, Numpy uses row-major operations. If you unroll an image to linearize the operation, you will end up with a row-major representation. Numpy can be configured to use column-major order, but this concern does not apply if you are using double for-loops without unrolling.\n",
    "\n",
    "**Hint**: To further inspect the shape of the objects and normal directions, consider using the `matplotlib.pyplot.quiver` function. Be sure to choose appropriate sub-sampling ratios for proper illustration. Add this to the `show_results()` function if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_surface(p, q, path_type='column'):\n",
    "    \"\"\"\n",
    "    Constructs the surface function represented as height_map.\n",
    "\n",
    "    This function integrates the partial derivatives p and q to reconstruct the surface height map.\n",
    "    The integration can be done along a column-major, row-major, or an averaged path.\n",
    "\n",
    "    Args:\n",
    "        p: The partial derivative of the surface function with respect to x (df/dx).\n",
    "        q: The partial derivative of the surface function with respect to y (df/dy).\n",
    "        path_type: The type of path to construct the height_map, either 'column', 'row', or 'average'.\n",
    "\n",
    "    Returns:\n",
    "        height_map: The reconstructed surface height map.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = p.shape\n",
    "    height_map = np.zeros([h, w])\n",
    "\n",
    "    if path_type == 'column':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif path_type == 'row':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    elif path_type == 'average':\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    return height_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, q, SE = check_integrability(normals_s25)\n",
    "\n",
    "height_map = construct_surface(p, q, \"column\")\n",
    "\n",
    "# set the threshold for visualization\n",
    "threshold = 0.01\n",
    "SE[SE <= threshold] = float('nan')\n",
    "\n",
    "print('Showing the surface reconstruction results for 5 light sources using column-major integration:')\n",
    "show_results(None, normals_s5, height_map, SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-23\"></a>\n",
    "#### <font color='#FF0000'>Question 23 (2 points)</font>\n",
    "\n",
    "Compare the results obtained from the two different integration paths (column-major and row-major). \n",
    "\n",
    "What are the differences in the reconstructed surface height maps when using these two paths? Discuss any observations you notice between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-24\"></a>\n",
    "#### <font color='#FF0000'>Question 24 (3 points)</font>\n",
    "\n",
    "Now, take the average of the results obtained from the column-major and row-major integration paths.\n",
    "\n",
    "Do you observe any improvement compared to using only one path? Additionally, analyze whether the construction results vary when using different numbers of images in the reconstruction process. (So compare the results obtained from the GraySphere5 and GraySphere25 datasets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-25\"></a>\n",
    "#### <font color='#FF0000'>Question 25 (3 points)</font>\n",
    "\n",
    "Run the photometric stereo algorithm on the MonkeyGray model using the `photometric_stereo` function and display the results. Complete the provided code, execute the algorithm, and show the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_stereo(image_dir='./photometrics_images/MonkeyGray/'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for a given set of images under varying illumination conditions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing the input images (default is './images/photometrics_images/MonkeyGray/').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_synthetic_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface gradient from the stack of images and light source matrix\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # threshold = ...\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal for MonkeyGrat, the cell below will run for at least 20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "image_dir = './photometrics_images/MonkeyGray/'\n",
    "photometric_stereo(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-26\"></a>\n",
    "#### <font color='#FF0000'>Question 26 (2 points)</font>\n",
    "\n",
    "The albedo results for the MonkeyGray model may exhibit more errors compared to the sphere. Observe and describe the errors that arise when running the photometric stereo algorithm on the MonkeyGray model.\n",
    "\n",
    "What could be the reasons for these errors? Experiment with different numbers of images, as you did in Question 15 and Question 18, to see the effects and discuss your observations.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-27\"></a>\n",
    "#### <font color='#FF0000'>Question 27 (2 points)</font>\n",
    "\n",
    "Considering the errors observed in the albedo results for the MonkeyGray model, what do you think could help in reducing or solving these errors? Provide your thoughts on potential strategies or methods that might improve the accuracy of the results.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-28\"></a>\n",
    "#### <font color='#FF0000'>Question 28 (5 points)</font>\n",
    "\n",
    "So far, we have assumed that albedos and input images are 1-channel grayscale images. To work with 3-channel RGB images, a simple approach is to split the input image into separate channels and treat them individually. However, this method may introduce issues when constructing the surface normal map if a pixel value in a channel is zero.\n",
    "\n",
    "Update the implementation to work with 3-channel RGB inputs. Complete the functions `load_synthetic_images_rgb` and `estimate_albedo_and_normals_rgb ` to handle RGB images, and test your updated implementation with the SphereColor and MonkeyColor models using the `photometric_stereo_rgb` function. Make sure to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_images_rgb(image_dir):\n",
    "    \"\"\"\n",
    "    Loads synthetic RGB images and light directions.\n",
    "\n",
    "    This function reads images from the specified directory, processes each color channel separately, \n",
    "    and stacks them together along with their corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the image stack and the light directions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load all image filenames\n",
    "    image_filenames = glob.glob(os.path.join(image_dir, \"*.png\"))  # Assuming the images are in PNG format\n",
    "    image_filenames.sort()  # Sorting to ensure proper order\n",
    "\n",
    "    # List to store images\n",
    "    images_rgb = []\n",
    "    \n",
    "    # Reading images and stacking them\n",
    "    for filename in image_filenames:\n",
    "        img = cv2.imread(filename)  # Reading as BGR image using OpenCV\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        images_rgb.append(img_rgb)\n",
    "\n",
    "    # Convert to numpy array (H, W, 3, num_images)\n",
    "    image_stack_rgb = np.stack(images_rgb, axis=-1)  # Shape: (H, W, 3, num_images)\n",
    "    \n",
    "    # Load light directions (assuming the format in the filename as described)\n",
    "    light_directions = []\n",
    "    z_component = 0.5  # Assumed z-component for light direction\n",
    "\n",
    "    for filename in image_filenames:\n",
    "        base_name = os.path.basename(filename)\n",
    "        x_component = np.double(base_name[(base_name.find('_') + 1):base_name.rfind('_')])\n",
    "        y_component = np.double(base_name[base_name.rfind('_') + 1:base_name.rfind('.png')])\n",
    "        light_directions.append([x_component, -y_component, z_component])\n",
    "\n",
    "    light_directions = np.array(light_directions)\n",
    "    \n",
    "    # Normalize light directions\n",
    "    norm_factors = np.linalg.norm(light_directions, axis=1, keepdims=True)\n",
    "    scriptV_rgb = light_directions / norm_factors\n",
    "\n",
    "    return image_stack_rgb, scriptV_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_albedo_and_normals_rgb(image_stack_rgb, scriptV_rgb):\n",
    "    \"\"\"\n",
    "    Estimates albedo and normals from a stack of RGB images and light directions.\n",
    "\n",
    "    This function processes each color channel separately to estimate the albedo and normal maps.\n",
    "    The results from each channel are then combined to produce the final albedo and normal maps.\n",
    "\n",
    "    Args:\n",
    "        image_stack: Image stacks for each color channel.\n",
    "        scriptV: Light directions for each color channel.\n",
    "        shadow_trick: Whether or not to use the shadow trick. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the albedo and normal maps.\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    height, width, _, num_images = image_stack_rgb.shape\n",
    "    \n",
    "    # Initialize arrays for storing albedo and normal maps\n",
    "    albedo_rgb = np.zeros((height, width, 3))  # Albedo for each channel\n",
    "    normals_rgb = np.zeros((height, width, 3, 3))  # Normals for each channel (x, y, z)\n",
    "\n",
    "    # Loop through each RGB channel and treat them individually\n",
    "    for channel in range(3):\n",
    "        # Extract the specific color channel (Red, Green, or Blue) across all images\n",
    "        image_stack_channel = image_stack_rgb[:, :, channel, :].reshape(-1, num_images)  # Shape: (H*W, num_images)\n",
    "        \n",
    "        # Solve for the surface normals and albedo for this specific channel\n",
    "        G, _, _, _ = np.linalg.lstsq(scriptV_rgb, image_stack_channel.T, rcond=None)  # Shape: (3, H*W)\n",
    "        \n",
    "        # G contains the components of the normal vectors and the albedo for this channel\n",
    "        albedo_channel = np.linalg.norm(G, axis=0)  # Albedo is the magnitude of G\n",
    "        normal_channel = G / (albedo_channel + 1e-8)  # Normals are G normalized by albedo (add small epsilon to avoid division by zero)\n",
    "        \n",
    "        # Reshape and store albedo and normals for this channel back into the final arrays\n",
    "        albedo_rgb[:, :, channel] = albedo_channel.reshape(height, width)  # Store in corresponding channel\n",
    "        normals_rgb[:, :, channel, :] = normal_channel.T.reshape(height, width, 3)  # Store in (R, G, B) channels\n",
    "\n",
    "    # Normalize the normal map across RGB channels\n",
    "    normals_avg = normals_rgb.mean(axis=2)  # Take the mean of the normals across RGB channels\n",
    "    normals_avg /= np.linalg.norm(normals_avg, axis=2, keepdims=True)  # Normalize the averaged normals\n",
    "    \n",
    "    return albedo_rgb, normals_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_stereo_rgb(image_dir='./SphereColor/'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo on RGB images to estimate the surface albedo, normals, and height map.\n",
    "\n",
    "    This function loads RGB images from the specified directory, computes the surface albedo and \n",
    "    normal maps, checks integrability, reconstructs the surface height map, and displays the results.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Path to the directory containing the images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load images and light directions using the provided function\n",
    "    image_stack_rgb, scriptV_rgb = load_synthetic_images_rgb(image_dir)\n",
    "\n",
    "    # Step 2: Estimate albedo and normals using the provided function\n",
    "    albedo_rgb, normals_rgb = estimate_albedo_and_normals_rgb(image_stack_rgb, scriptV_rgb)\n",
    "\n",
    "    # Step 3: Check integrability of normals\n",
    "    p, q, SE = check_integrability(normals_rgb)\n",
    "\n",
    "    # Construct the height map\n",
    "    height_map = construct_surface(p, q)\n",
    "\n",
    "    # Step 5: Display results\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Display albedo map\n",
    "    axs[0].imshow(albedo_rgb / np.max(albedo_rgb))  # Normalize for display\n",
    "    axs[0].set_title('Albedo')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Display normals (shifted to [0, 1] for display purposes)\n",
    "    normals_rgb_disp = (normals_rgb + 1) / 2\n",
    "    axs[1].imshow(normals_rgb_disp)\n",
    "    axs[1].set_title('Normals')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Display reconstructed height map\n",
    "    axs[2].imshow(height_map, cmap='gray')\n",
    "    axs[2].set_title('Constructed Height Map')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Display the integrability check\n",
    "    plt.figure()\n",
    "    plt.imshow(SE, cmap='hot')\n",
    "    plt.colorbar()\n",
    "    plt.title('Integrability Check (SE)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal for SphereColor and MonkeyColor, the cell below will run for atleast 1 or 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometric_stereo_rgb(image_dir='./photometrics_images/SphereColor/')\n",
    "photometric_stereo_rgb(image_dir='./photometrics_images/MonkeyColor/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-29\"></a>\n",
    "#### <font color='#FF0000'>Question 29 (2 points)</font>\n",
    "\n",
    "Explain the changes made to the function and discuss the results of these changes.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-30\"></a>\n",
    "#### <font color='#FF0000'>Question 30 (2 points)</font>\n",
    "\n",
    "Observe the problem in the constructed surface normal map and height map. Explain why a zero pixel could be a problem and propose a way to overcome that.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-31\"></a>\n",
    "#### <font color='#FF0000'>Question 31 (4 points)</font>\n",
    "\n",
    "Now, it's time to apply the algorithm to real-world datasets. We will first use the Yale Face Database.\n",
    "\n",
    "For the Face dataset, you should:\n",
    "- Load the images\n",
    "- Compute the surface albedo and normal map\n",
    "- Run the integrability check\n",
    "- Find the number of outliers\n",
    "- Compute the surface height\n",
    "- Show the results\n",
    "\n",
    "Run the algorithm for the Yale Face images: [Yale Face Database](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html). The Yale face data is included in the lab material. The `load_face_images` function is provided as a helper. You should complete the `photometric_stereo_face_dataset` function as part of this task.\n",
    "\n",
    "Make sure to display the results for the Face dataset using the 3 paths (column-major, row-major, and average) when constructing the surface height map.\n",
    "\n",
    "**Hint**: For proper computation of albedo and surface normal, you may want to suspend the shadow trick described in the text, and use the original formula:\n",
    "$$i = Vg(x,y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_face_images(image_dir='./photometrics_images/yaleB02/'):\n",
    "    \"\"\"\n",
    "    Loads a set of face images from the Yale Face Database and computes corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: The directory containing the Yale Face images. Defaults to './photometrics_images/yaleB02/'.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - image_stack: A stack of images with the ambient image subtracted and normalized.\n",
    "            - scriptV: An array of light direction vectors corresponding to each image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the number of images to load\n",
    "    num_images = 64\n",
    "    \n",
    "    # load the ambient image (the image without any lighting)\n",
    "    filename = os.path.join(image_dir, 'yaleB02_P00_Ambient.pgm')\n",
    "    ambient_image = cv2.imread(filename, -1)\n",
    "    \n",
    "    # get the height and width of the ambient image\n",
    "    h, w = ambient_image.shape\n",
    "\n",
    "    # import glob to find all image files in the directory\n",
    "    import glob\n",
    "    \n",
    "    # get a list of all image files that match the pattern for the yaleB02 face dataset\n",
    "    d = glob.glob(os.path.join(image_dir, 'yaleB02_P00A*.pgm'))\n",
    "    \n",
    "    # randomly select a subset of images to match the num_images count\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    \n",
    "    # extract the base filenames from the list of image paths\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "\n",
    "    # initialize arrays for storing angles and image data\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    # loop through each selected image file\n",
    "    for j in range(num_images):\n",
    "        # extract the lighting angles from the filename\n",
    "        ang[0,j], ang[1,j] = np.double(filenames[j][12:16]), np.double(filenames[j][17:20])\n",
    "        \n",
    "        # load the image and subtract the ambient image to get the actual illumination effect\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1) - ambient_image\n",
    "\n",
    "    # calculate the light direction vectors based on the extracted angles\n",
    "    x = np.cos(np.pi*ang[1,:]/180) * np.cos(np.pi*ang[0,:]/180)\n",
    "    y = np.cos(np.pi*ang[1,:]/180) * np.sin(np.pi*ang[0,:]/180)\n",
    "    z = np.sin(np.pi*ang[1,:]/180)\n",
    "    \n",
    "    # combine the direction vectors, adjusting for normal direction\n",
    "    scriptV = [-1,-1,1] * np.array([y,z,x]).transpose(1,0)  ##eggonz COMMENT: fix (-1,-1,1) normal directiona\n",
    "\n",
    "    # convert the image stack to double precision\n",
    "    image_stack = np.double(image_stack)\n",
    "    \n",
    "    # set any negative pixel values to 0 (removing underflow)\n",
    "    image_stack[image_stack<0] = 0\n",
    "    \n",
    "    # find the minimum and maximum pixel values in the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    \n",
    "    # normalize the image stack to have values between 0 and 1, or set to zeros if min and max are equal\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  # avoid failure when all pixel values are the same\n",
    "\n",
    "    # return the processed image stack and light directions\n",
    "    return image_stack, scriptV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_stereo_face_dataset(image_dir='./images/photometrics_images/yaleB02/', path_type='average'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for the Yale Face Dataset under varying illumination conditions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing the input images (default is './images/photometrics_images/yaleB02/').\n",
    "        path_type: Type of path integration to use for surface height computation (default is 'average').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_face_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface albedo and normal map\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # threshold = ...\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height map\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal using the three path types, the cell below will run for at least 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-32\"></a>\n",
    "#### <font color='#FF0000'>Question 32 (1 point)</font>\n",
    "\n",
    "Observe and discuss the results for different integration paths in the `photometric_stereo_face_dataset` function.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-33\"></a>\n",
    "#### <font color='#FF0000'>Question 33 (2 points)</font>\n",
    "\n",
    "Discuss how the images violate the assumptions of the shape-from-shading methods. Include specific input images to illustrate your points.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-34\"></a>\n",
    "#### <font color='#FF0000'>Question 34 (2 points)</font>\n",
    "\n",
    "How would the results improve when the problematic images are removed? Try it out and show the results in your notebook.\n",
    "\n",
    "To complete this task, you should implement the `load_face_images_filtered` and `photometric_stereo_face_filtered` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_face_images_filtered(image_dir='./photometrics_images/yaleB02/', bad_images=[]):\n",
    "    \"\"\"\n",
    "    Loads a set of face images from the Yale Face Database, excluding problematic ones, and computes corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: The directory containing the Yale Face images. Defaults to './photometrics_images/yaleB02/'.\n",
    "        bad_images: A list of filenames to exclude from the image stack.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - image_stack: A stack of images with the ambient image subtracted, normalized, and problematic images removed.\n",
    "            - scriptV: An array of light direction vectors corresponding to each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return image_stack, scriptV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_stereo_face_filtered(bad_images=[], image_dir='./photometrics_images/yaleB02/', shadow_trick=False, path_type='average'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for the Yale Face Dataset, excluding problematic images.\n",
    "\n",
    "    Args:\n",
    "        bad_images: A list of filenames to exclude from the image stack (default is an empty list).\n",
    "        image_dir: Directory containing the input images (default is './photometrics_images/yaleB02/').\n",
    "        shadow_trick: Whether or not to use the shadow trick for albedo and normal estimation (default is False).\n",
    "        path_type: Type of path integration to use for surface height computation (default is 'average').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-35\"></a>\n",
    "#### <font color='#FF0000'>Question 35 (1 point)</font>\n",
    "\n",
    "Discuss the results obtained after removing the problematic images. What are the differences in the results, and how do they compare to the original results?\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-36\"></a>\n",
    "#### <font color='#FF0000'>Question 36 (4 points)</font>\n",
    "\n",
    "Now, it's time to apply the algorithm to the Apple dataset using real-world 3-channel RGB inputs.\n",
    "\n",
    "For the Apple dataset, you should:\n",
    "- Load the images\n",
    "- Compute the surface albedo and normal map\n",
    "- Run the integrability check\n",
    "- Find the number of outliers\n",
    "- Compute the surface height\n",
    "- Show the results\n",
    "\n",
    "Use the images in the \"Apple\" folder. The `load_apple_images` function is provided as a helper. You should complete the `photometric_stereo_apple_dataset` function as part of this task.\n",
    "\n",
    "Observe and discuss the results for different integration paths. You may encounter difficulties using this non-synthetic dataset—try if filtering may help.\n",
    "\n",
    "Make sure to display the results using the 3 paths (column-major, row-major, and average) when constructing the surface height map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_apple_images(image_dir='./photometrics_images/Apple'):\n",
    "    \"\"\"\n",
    "    Loads a set of apple images from the Apple dataset and computes corresponding light directions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: The directory containing the Apple images. Defaults to './photometrics_images/Apple/'.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - image_stack: A stack of images with only the red channel extracted and normalized.\n",
    "            - scriptV: An array of light direction vectors corresponding to each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # define the number of images to load\n",
    "    num_images = 99\n",
    "    \n",
    "    # load a sample image to determine its dimensions\n",
    "    filename = os.path.join(image_dir, 'I_0000.png')\n",
    "    try_image = cv2.imread(filename, -1)\n",
    "    \n",
    "    # get the height and width of the sample image\n",
    "    h, w = try_image[:,:,0].shape\n",
    "\n",
    "    # import glob to find all image files in the directory\n",
    "    import glob\n",
    "    \n",
    "    # get a list of all image files that match the pattern for the Apple dataset\n",
    "    d = glob.glob(os.path.join(image_dir, 'I_00*.png'))\n",
    "    \n",
    "    # randomly select a subset of images to match the num_images count\n",
    "    import random\n",
    "    d = random.sample(d, num_images)\n",
    "    \n",
    "    # extract the base filenames from the list of image paths and their corresponding indices\n",
    "    filenames = [os.path.basename(x) for x in d]\n",
    "    filenames_idx = [int(i.split('_')[1].split('.')[0]) for i in filenames]\n",
    "\n",
    "    # initialize arrays for storing angles and image data\n",
    "    ang = np.zeros([2, num_images])\n",
    "    image_stack = np.zeros([h, w, num_images])\n",
    "\n",
    "    # loop through each selected image file\n",
    "    for j in range(num_images):\n",
    "        # load the image and extract the red channel (index 2)\n",
    "        image_stack[...,j] = cv2.imread(os.path.join(image_dir, filenames[j]), -1)[:,:,2]\n",
    "\n",
    "    # read light direction vectors from the provided file\n",
    "    with open('./photometrics_images/Apple/light_directions_refined.txt') as file:\n",
    "        lines = [line.split() for line in file]\n",
    "        x, y, z = [], [], []\n",
    "        \n",
    "        # extract light directions corresponding to the selected images\n",
    "        for idx in filenames_idx:\n",
    "            x.append(float(lines[idx][0]))\n",
    "            y.append(float(lines[idx][1]))\n",
    "            z.append(float(lines[idx][2]))\n",
    "\n",
    "    # combine the direction vectors, adjusting for normal direction\n",
    "    scriptV = [1,-1,1] * np.array([x,y,z]).transpose(1,0)  ##eggonz COMMENT: fix (1,-1,1) normal directiona\n",
    "\n",
    "    # convert the image stack to double precision\n",
    "    image_stack = np.double(image_stack)\n",
    "    \n",
    "    # set any negative pixel values to 0 (removing underflow)\n",
    "    image_stack[image_stack < 0] = 0\n",
    "    \n",
    "    # find the minimum and maximum pixel values in the image stack\n",
    "    min_val = np.min(image_stack)\n",
    "    max_val = np.max(image_stack)\n",
    "    \n",
    "    # normalize the image stack to have values between 0 and 1, or set to zeros if min and max are equal\n",
    "    if max_val != min_val:\n",
    "        image_stack = (image_stack - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        image_stack = np.zeros(image_stack.shape)  # avoid failure when all pixel values are the same\n",
    "\n",
    "    # return the processed image stack and light directions\n",
    "    return image_stack, scriptV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photometric_stereo_apple_dataset(image_dir='./photometrics_images/Apple/', path_type='average'):\n",
    "    \"\"\"\n",
    "    Performs photometric stereo to estimate the surface albedo, normals, and height map for the Apple Dataset under varying illumination conditions.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory containing the input images (default is './photometrics_images/Apple/').\n",
    "        path_type: Type of path integration to use for surface height computation (default is 'average').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain many images in a fixed view under different illumination\n",
    "    print('Loading images...\\n')\n",
    "    [image_stack, scriptV] = load_apple_images(image_dir)\n",
    "    [h, w, n] = image_stack.shape\n",
    "    print('Finish loading %d images.\\n' % n)\n",
    "\n",
    "    # compute the surface albedo and normal map\n",
    "    print('Computing surface albedo and normal map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # integrability check: is (dp / dy  -  dq / dx) ^ 2 small everywhere?\n",
    "    print('Integrability checking\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # threshold = ...\n",
    "\n",
    "\n",
    "    print('Number of outliers: %d\\n' % np.sum(SE > threshold))\n",
    "    SE[SE <= threshold] = float('nan')  # for good visualization\n",
    "\n",
    "    # compute the surface height map\n",
    "    print('Computing surface height map...\\n')\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # show results\n",
    "    print('Displaying results...\\n')\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you estimate the albedo and surface normal using the three path types, the cell below will run for at least 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"question-37\"></a>\n",
    "#### <font color='#FF0000'>Question 37 (1 point)</font>\n",
    "\n",
    "Observe and discuss the results for different integration paths in the `photometric_stereo_apple_dataset` function.\n",
    "\n",
    "##### <font color='yellow'>Answer:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "04d82e9e516e4497f3a1c5c230627fea35db5058fe4d8e2f26fb03c7c1937ddb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
